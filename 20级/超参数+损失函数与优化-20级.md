



设置超参数策略：<br />![](https://cdn.nlark.com/yuque/0/2021/png/21571571/1621082269477-61a7021e-6598-43c9-8306-e91e21daa088.png#)

1. 要考虑算法在未知情况下的表现情况 ， 根据验证集和测试集共同决定超参数的测试

2. k折交叉验证



KNN 为什么不适用与图像处理：

Knn 在分类时， 随着数据的维度增加 ，需要更多地数据<br />L2 距离（欧几里得距离不能很好的衡量图像之间的相似性）即具体表现再对一些变换后，你仍然可以得到相同地L2距离。



线性分类：Linear Classification<br />![](https://cdn.nlark.com/yuque/0/2021/png/21571571/1621082270192-52ab9dba-2093-42b9-8972-8d495c8de714.png#)<br />对于一个f(x ,W) ,我们要做的就是找到合适的W（一个参数矩阵）然后把他保存下来 ，这就是我们线性分类的“模型”。

我们要把 每个通道的每个像素全拿出来 ， 对每一个点适配一个权重 ，最红输出一个（Num_class , 1）的矩阵 ， 之后取最大值 ，就是可能的分类了。<br />关于偏置项b ：（假如说是一个十分类的任务）b 是一个 10 X 1 的矩阵，里面全是常数 ， 它不与训练数据交互 ， 当数据不平衡时 ，可以考虑增加矩阵对应元素的值。


线性分类 无法应对多态问题，一个类别的多种状态 ，线性分类无法很好得work。



损失函数：<br />![](https://cdn.nlark.com/yuque/0/2021/png/21571571/1621082270851-e66472bc-2174-472e-8af1-2b55bd1db9f9.png#)

1. 合页损失函数

1. 损失函数为什么有时候设置平方还是非平方的问题 ： 平方项可以扩大你的错误 ， 向上图中Hinge loss 是为了忽略微小的错误 ，当错误累计到一定阈值后，进行统计。

![](https://cdn.nlark.com/yuque/0/2021/png/21571571/1621082271269-56e0fecf-2314-4a8a-bfbd-f1aad3163b12.png#)<br />损失函数加入正则化项 ，鼓励分类器选择更简单的参数矩阵 ，缓解过拟合。

L1正则化 ，鼓励权值矩阵变得更稀疏一些 ，L2则会让权值矩阵变得平均一些。


Softmax损失函数：<br />![](https://cdn.nlark.com/yuque/0/2021/png/21571571/1621082271833-ad7962e3-c90c-4901-8163-f8d13f4dc5fc.png#)

P是每个类的概率 ， 得到多行向量每个元素和为一。优化目标是尽可能让你期望的那个概率尽可能地接近1。

式子中用e做底数 ，是为了只让正数参与运算 ， 分母可以理解为一个“归一化”的操作。<br />![](https://cdn.nlark.com/yuque/0/2021/png/21571571/1621082272255-924edc42-a5d8-4bb0-b048-bb4a1a413936.png#)-log（小数）得到的是正数。<br />![](https://cdn.nlark.com/yuque/0/2021/png/21571571/1621082272975-69e6979a-7cb7-4eca-afb0-5a42632697ef.png#)

Softmax 最大值是正无穷 ， 最小值是0 ， 当正确预测时 概率为1 ， log 作用后为0 ， 当预测的很离谱是时 ， 概率趋于0 ， -log 后就是正无穷。

--------一个问题 ， 在P7介绍的softmax 中 ，当s趋于0 ， 时 损失函数为什么是ln（c）




Optimization：<br />新的W = 旧的W – learningrate * Gradient。

随机梯度下降：<br />并非计算整个训练集的<br />误差和梯度值 ， 而是每次迭代选取小部分的训练样本。
